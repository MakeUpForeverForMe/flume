# 各组件的命名
a.sources = taildirSource
a.channels = memoryChannel
a.sinks = hdfsSink



# 指定source和sink的channel
a.sources.taildirSource.channels = memoryChannel
a.sinks.hdfsSink.channel = memoryChannel

#------- taildirSource相关配置-------------------------

a.sources.taildirSource.type = TAILDIR
a.sources.taildirSource.filegroups = logSource
a.sources.taildirSource.filegroups.logSource = /opt/module/log/CollectLogs.log

#------- source拦截器相关配置  -------------------------
# 定义拦截器
a.sources.taildirSource.interceptors = i1 i2
# regex_filter 过滤数据
a.sources.taildirSource.interceptors.i1.type = regex_filter
# 过滤数据
a.sources.taildirSource.interceptors.i1.regex = "type":"(\\w+)"

# regex_extractor 将相应的数据抽取出来, 在头文件中添加相应信息
a.sources.taildirSource.interceptors.i2.type = regex_extractor
# 设置正则表达式，匹配指定的数据，这样设置会在数据的header中增加log_type="某个值"
a.sources.taildirSource.interceptors.i2.regex = "type":"(\\w+)"
a.sources.taildirSource.interceptors.i2.serializers = s1
a.sources.taildirSource.interceptors.i2.serializers.s1.name = log_type

#------- memoryChannel相关配置-------------------------

a.channels.memoryChannel.type = memory
a.channels.memoryChannel.capacity = 1000
a.channels.memoryChannel.transactionCapacity = 1000
a.channels.memoryChannel.byteCapacityBufferPercentage = 20
a.channels.memoryChannel.byteCapacity = 800000

#-------  hdfsSink相关配置    -------------------------

a.sinks.hdfsSink.type = hdfs
a.sinks.hdfsSink.hdfs.path = hdfs://hadoop102:9000/flume-hdfs/%Y-%m-%d
# 写入的文件的格式(默认Writable.), 写入HDFS时需要修改为Text, 否则Hive无法读取这些文件
a.sinks.hdfsSink.hdfs.writeFormat = Text
# 文件格式 : SequenceFile(默认)、DataStream、CompressedStream
# DataStream输出的是不可被压缩文件; CompressedStream输出的是可压缩文件, 且需要设置压缩格式
a.sinks.hdfsSink.hdfs.fileType = DataStream
# 设置压缩格式, 可用压缩格式 : gzip, bzip2, lzo, lzop, snappy
# a.sinks.hdfsSink.hdfs.codeC = 
# 开启时间戳，在头文件中添加时间戳
a.sinks.hdfsSink.hdfs.useLocalTimeStamp = true
# 配置前缀和后缀
a.sinks.hdfsSink.hdfs.filePrefix = %{log_type}
a.sinks.hdfsSink.hdfs.fileSuffix = .log
# 一天滚动一个文件夹
# 开启时间戳向下舍入(除％t之外的所有基于时间的转义序列)
a.sinks.hdfsSink.hdfs.round = true
# 时间戳舍入的范围一天
a.sinks.hdfsSink.hdfs.roundValue = 24
# 只有second、minute、hour三种格式
a.sinks.hdfsSink.hdfs.roundUnit = hour
# 每隔一小时(默认30s)将临时文件滚动成一个目标文件
a.sinks.hdfsSink.hdfs.rollInterval = 3600
# 当文件大小为128M时，将临时文件滚动成一个目标文件(默认1024B)
a.sinks.hdfsSink.hdfs.rollSize = 134217728
# 事件数据达到该数量的时候，将临时文件滚动成目标文件(从不基于事件滚动,默认10个事件)
a.sinks.hdfsSink.hdfs.rollCount = 0
# 在将文件刷新到HDFS之前写入文件的事件数(默认100)
a.sinks.hdfsSink.hdfs.batchSize = 600
# 指定每个HDFS块的最小副本数。如果未指定，则它来自类路径中的默认Hadoop配置
# a.sinks.hdfsSink.hdfs.minBlockReplicas = 1
# 设置hdfs的超时访问时间(默认10000毫秒)
# a.sinks.hdfsSink.hdfs.callTimeout = 3600000





# a.sinkgroups = g1
# a.sinks = k1 k2
a.sinkgroups.g1.sinks = k1 k2
a.sinkgroups.g1.processor.type = load_balance
a.sinkgroups.g1.processor.backoff = true
a.sinkgroups.g1.processor.selector = round_robin
a.sinkgroups.g1.processor.selector.maxTimeOut = 10000

a.sinks.k1.type = avro
a.sinks.k1.channel = memoryChannel
a.sinks.k1.batchSize = 1
a.sinks.k1.hostname = hadoop102
a.sinks.k1.port = 1234

a.sinks.k2.type = avro
a.sinks.k2.channel = memoryChannel
a.sinks.k2.batchSize = 1
a.sinks.k2.hostname = hadoop102
a.sinks.k2.port = 4321



a.sources.r1.type = avro
a.sources.r1.bind = 192.168.1.102
a.sources.r1.port = 1234
a.sources.r1.channels = memoryChannel

a.sources.r1.type = avro
a.sources.r1.bind = 192.168.1.102
a.sources.r1.port = 4321
a.sources.r1.channels = memoryChannel